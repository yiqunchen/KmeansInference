% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kmeans_inf.R
\name{kmeans_inference}
\alias{kmeans_inference}
\title{Testing for a difference in means between clusters of observations
identified via k-means clustering.}
\usage{
kmeans_inference(
  X,
  k,
  cluster_1,
  cluster_2,
  iso = TRUE,
  sig = NULL,
  SigInv = NULL,
  iter.max = 10,
  seed = 1234,
  tol_eps = 1e-06,
  verbose = TRUE
)
}
\arguments{
\item{X}{Numeric matrix; \eqn{n} by \eqn{q} matrix of observed data}

\item{k}{Integer; the number of clusters for k-means clustering}

\item{cluster_1, cluster_2}{Two different integers in {1,...,k}; two estimated clusters to test, as indexed by the results of
\code{kmeans_estimation}.}

\item{iso}{Boolean. If TRUE, an isotropic covariance matrix model is used.}

\item{sig}{Numeric; noise standard deviation for the observed data, a non-negative number;
relevant if \code{iso}=TRUE.}

\item{SigInv}{Numeric matrix; optional \eqn{q} by \eqn{q} matrix specifying \eqn{\Sigma^{-1}}; relevant if \code{iso} is FALSE.}

\item{iter.max}{Positive integer;     the maximum number of iterations allowed in the k-means clustering algorithm. Default to \code{10}.}

\item{seed}{Random seed for the initialization in k-means clustering algorithm.}

\item{tol_eps}{A small number specifying the convergence criterion for the k-means clustering,
default to \code{1e-6}.}
}
\value{
Returns a list with the following elements:
\itemize{
\item \code{pval} the selective p-value \eqn{p_{k-means}} in Chen and Witten (2022+)
\item \code{final_interval} the conditioning set of Chen and Witten (2022+), stored as the \code{Intervals} class
\item \code{test_stats} test statistic: the difference in the empirical means of two estimated clusters
\item \code{final_cluster} Estimated clusters via k-means clustering
}
}
\description{
This functions tests the null hypothesis of no difference in means between
two estimated clusters \code{cluster_1} and \code{cluster_2} of the output of the
k means clustering solution obtained via the Lloyd's algorithm.
The ordering are numbered as per the results of the \code{kmeans_estimation}
function in the \code{KmeansInference} package.
}
\details{
Consider the generative model \eqn{X \sim MN(\mu,I_n,\sigma^2 I_q)}, The k-means clustering
solves the following optimization problem
\deqn{ \sum_{k=1}^K \sum_{i \in \mathcal{C}_k} \left\Vert x_i -  \frac{\sum_{i \in \mathcal{C}_k} x_i}{|\mathcal{C}_k|}
 \right\Vert_2^2 , }
 where \eqn{\mathcal{C}_1,..., {\mathcal{C}_K}} forms a partition of the integers \eqn{1,..., n}, and can be regarded as
 the estimated clusters of the original observations. In practice, solutions to the optimization problem is
 often obtained using iterative algorithms, e.g., the Lloyd's algorithm.
Now suppose we want to test whether the means of two estimated clusters \code{cluster_1} and \code{cluster_2}
are equal; or equivalently, the null hypothesis of the form \eqn{H_{0}:  \mu^T \nu = 0_q} versus
\eqn{H_{1}:   \mu^T \nu \neq 0_q} for suitably chosen \eqn{\nu} and all-zero vectors \eqn{0_q}.

This function computes the following p-value:
\deqn{P \left( \left\Vert X^T \nu \right\Vert_2 \ge \left\Vert x^T \nu  \right\Vert_2 \; | \;
  \bigcap_{t=1}^{T}\bigcap_{i=1}^{n} \left\{ c_i^{(t)} \left( X \right) =
 c_i^{(t)}\left( x \right) \right\},  \Pi_\nu^\perp Y  =  \Pi_\nu^\perp y \right),}
where \eqn{c_i^{(t)}} is the is the cluster assigned to the \eqn{i}th observation at the \eqn{t}th iteration of
the Lloyd's algorithm, and \eqn{\Pi_\nu^\perp} is the orthogonal projection to the orthogonal complement of \eqn{\nu}.
In particular, the test based on this p-value controls the selective Type I error and has substantial power.
Readers can refer to the Sections 2 and 4 in Chen and Witten (2022+) for more details.
}
\examples{
library(KmeansInference)
library(ggplot2)
set.seed(2022)
n <- 150
true_clusters <- c(rep(1, 50), rep(2, 50), rep(3, 50))
delta <- 10
q <- 2
mu <- rbind(c(delta/2,rep(0,q-1)),
c(rep(0,q-1), sqrt(3)*delta/2),
c(-delta/2,rep(0,q-1)) )
sig <- 1
# Generate a matrix normal sample
X <- matrix(rnorm(n*q, sd=sig), n, q) + mu[true_clusters, ]
# Visualize the data
ggplot(data.frame(X), aes(x=X1, y=X2)) +
geom_point(cex=2) + xlab("Feature 1") + ylab("Feature 2") +
 theme_classic(base_size=18) + theme(legend.position="none") +
 scale_colour_manual(values=c("dodgerblue3", "rosybrown", "orange")) +
 theme(legend.title = element_blank(),
 plot.title = element_text(hjust = 0.5))
 k <- 3
 # Run k-means clustering with K=3
 estimated_clusters <- kmeans_estimation(X, k,iter.max = 20,seed = 2021)$final_cluster
 table(true_clusters,estimated_clusters)
 # Visualize the clusters
 ggplot(data.frame(X), aes(x=X1, y=X2, col=as.factor(estimated_clusters))) +
 geom_point(cex=2) + xlab("Feature 1") + ylab("Feature 2") +
 theme_classic(base_size=18) + theme(legend.position="none") +
 scale_colour_manual(values=c("dodgerblue3", "rosybrown", "orange")) +
 theme(legend.title = element_blank(), plot.title = element_text(hjust = 0.5))
 ### Run a test for a difference in means between estimated clusters 1 and 3
 cluster_1 <- 1
 cluster_2 <- 3
 cl_1_2_inference_demo <- kmeans_inference(X, k=3, cluster_1, cluster_2,
 sig=sig, iter.max = 20, seed = 2021)
 summary(cl_1_2_inference_demo)
}
\references{
Chen YT, Witten DM. (2022+) Selective inference for k-means clustering. arXiv preprint.
https://arxiv.org/abs/xxxx.xxxxx.
Lloyd, S. P. (1957, 1982). Least squares quantization in PCM. Technical Note, Bell Laboratories.
Published in 1982 in IEEE Transactions on Information Theory, 28, 128â€“137.
}
